{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Elliot Richardson\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "I. [Problem Statement](#Problem-Statement)\n",
    "\n",
    "II. [Data Dictionary](#Data-Used)\n",
    "\n",
    "III. [Exploratory Analysis](#Exploratory-analysis)\n",
    "\n",
    "IV. [Modeling](#Modeling)\n",
    "\n",
    "V. [Model Selection](#Model-Selection)\n",
    "\n",
    "VI. [Findings](#Findings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "For this project, I'm an analyst at a political targeting firm. In light of the pandemic, a larger share of direct voter contact is over the internet, rather than in person. However, it isn't easy to connect social media users to their voter file records and therefore their ideological support and voter turnout scores as determined by other models. So, in order to identify persuadable targets and activatable supporters, my firm wants to create a model that can differentiate between language used by people on varying ends of the political spectrum. Eventually, they will use this model to identify target accounts on platforms like Twitter to whom campaigns will conduct outreach. \n",
    "\n",
    "**Question I'm seeking to answer:** Do Reddit users on varying ends of the economic political spectrum use vocabularies distinct enough for a model to differentiate between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Used\n",
    "\n",
    "Titles and content of posts requested from [r/Socialism](https://www.reddit.com/r/DemocraticSocialism/) and [r/Capitalism](https://www.reddit.com/r/Capitalism/) using the [Pushshift API](https://pushshift.io/).\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "|Feature|Type|Description|\n",
    "|---|---|---|\n",
    "|subreddit_s|int64|Binary indicator of the subreddit from which a post was pulled (1 = r/Socialism, 0 = r/Capitalism)|\n",
    "|sentences|int64|Number of sentences in post as determined by nltk's PunktSentenceTokenizer|\n",
    "|avg_sent_len|float64|Average number of words in each sentences of the post|\n",
    "|words|int64|Total number of words in the post|\n",
    "|avg_word_len|float64|Average number of characters in each word of the post|\n",
    "|ADJ_prop|float64|Proportion of the content made up of adjectives|\n",
    "|ADP_prop|float64|Proportion of the content made up of adpositions|\n",
    "|ADV_prop|float64|Proportion of the content made up of adverbs|\n",
    "|AUX_prop|float64|Proportion of the content made up of auxiliary words|\n",
    "|CCONJ_prop|float64|Proportion of the content made up of coordinating conjunctions|\n",
    "|DET_prop|float64|Proportion of the content made up of determiners|\n",
    "|INTJ_prop|float64|Proportion of the content made up of interjections|\n",
    "|NOUN_prop|float64|Proportion of the content made up of nouns|\n",
    "|NUM_prop|float64|Proportion of the content made up of numbers|\n",
    "|PART_prop|float64|Proportion of the content made up of particles|\n",
    "|PRON_prop|float64|Proportion of the content made up of pronouns|\n",
    "|PROPN_prop|float64|Proportion of the content made up of proper nouns|\n",
    "|PUNCT_prop|float64|Proportion of the content made up of punctuation|\n",
    "|SCONJ_prop|float64|Proportion of the content made up of subordinating conjunction|\n",
    "|SPACE_prop|float64|Proportion of the content made up of spaces|\n",
    "|SYM_prop|float64|Proportion of the content made up of symbols|\n",
    "|VERB_prop|float64|Proportion of the content made up of verbs|\n",
    "|X_prop|float64|Proportion of the content made up of uncategorizable words|\n",
    "|vader_neg|float64|Average negativity score for the words in the post as determined by vaderSentiment's SentimentIntensityAnalyzer|\n",
    "|vader_pos|float64|Average positivity score for the words in the post as determined by vaderSentiment's SentimentIntensityAnalyzer|\n",
    "|vader_neu|float64|Average neutrality score for the words in the post as determined by vaderSentiment's SentimentIntensityAnalyzer|\n",
    "|vader_compound|float64|Average compound score for the words in the post as determined by vaderSentiment's SentimentIntensityAnalyzer|\n",
    "\n",
    "Check the documentation for [spaCy](https://spacy.io/api/annotation) and [vaderSentiment](https://github.com/cjhutto/vaderSentiment#resources-and-dataset-descriptions) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data and importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib_venn import venn2\n",
    "from matplotlib.dates import date2num\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, \\\n",
    "        GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5847, 1224)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_s</th>\n",
       "      <th>sentences</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>words</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>ADJ_prop</th>\n",
       "      <th>ADP_prop</th>\n",
       "      <th>ADV_prop</th>\n",
       "      <th>AUX_prop</th>\n",
       "      <th>CCONJ_prop</th>\n",
       "      <th>...</th>\n",
       "      <th>wrote</th>\n",
       "      <th>www</th>\n",
       "      <th>ye</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yolk</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>zapatista</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>114</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>17.466667</td>\n",
       "      <td>262</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>0.071186</td>\n",
       "      <td>0.105085</td>\n",
       "      <td>0.108475</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.020339</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>180</td>\n",
       "      <td>4.458333</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.069652</td>\n",
       "      <td>0.034826</td>\n",
       "      <td>0.049751</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1224 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit_s  sentences  avg_sent_len  words  avg_word_len  ADJ_prop  \\\n",
       "0            1          6     19.000000    114      3.466667  0.064000   \n",
       "1            1          4      7.000000     28      2.000000  0.125000   \n",
       "2            1          1     30.000000     30      5.000000  0.000000   \n",
       "3            1         15     17.466667    262      4.214286  0.071186   \n",
       "4            1          8     22.500000    180      4.458333  0.074627   \n",
       "\n",
       "   ADP_prop  ADV_prop  AUX_prop  CCONJ_prop  ...  wrote  www  ye  yeah  year  \\\n",
       "0  0.080000  0.056000  0.072000    0.024000  ...      0    0   0     0     1   \n",
       "1  0.125000  0.000000  0.031250    0.031250  ...      0    0   0     0     0   \n",
       "2  0.100000  0.133333  0.066667    0.100000  ...      0    0   0     0     0   \n",
       "3  0.105085  0.108475  0.050847    0.020339  ...      0    0   0     0     0   \n",
       "4  0.119403  0.069652  0.034826    0.049751  ...      0    0   0     0     0   \n",
       "\n",
       "   yolk  young  youtub  zapatista  zero  \n",
       "0     0      0       1          0     0  \n",
       "1     0      0       0          0     0  \n",
       "2     0      0       0          0     0  \n",
       "3     0      0       5          0     0  \n",
       "4     0      0       0          0     0  \n",
       "\n",
       "[5 rows x 1224 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/model_df.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonword_cols = list(df.columns[:27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(df[nonword_cols].corr()[['subreddit_s']].sort_values('subreddit_s')[:-1],\n",
    "            vmax=1,\n",
    "            vmin=-1,\n",
    "            annot=True,\n",
    "            cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so that isn't particularly encouraging, but hopefully the words will have stronger effects. I just couldn't fit all 1200 in the correlation heatmap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s_words = set(df.drop(columns=nonword_cols)[df['subreddit_s']==1].sum().sort_values(ascending=False)[:100].index)\n",
    "c_words = set(df.drop(columns=nonword_cols)[df['subreddit_s']==0].sum().sort_values(ascending=False)[:100].index)\n",
    "\n",
    "Sc = len(s_words.difference(c_words))\n",
    "sC = len(c_words.difference(s_words))\n",
    "SC = len(c_words.intersection(s_words))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Overlap Between Top 100 Most Common Words',fontsize=16)\n",
    "venn2(subsets = (Sc, sC, SC),\n",
    "      set_labels = ('Socialism', 'Capitalism'),\n",
    "      set_colors=('indigo', 'skyblue'),\n",
    "      alpha = 0.6);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely a little bit intimidating! I might not have chosen subreddits with different enough vocabularies, but I wanted to do something that might actually be useful haha. We'll see how it plays out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc_mentions_in_soc = df.loc[df['subreddit_s']==1,'socialis'].sum()\n",
    "soc_mentions_in_cap = df.loc[df['subreddit_s']==0,'socialis'].sum()\n",
    "\n",
    "cap_mentions_in_soc = df.loc[df['subreddit_s']==1,'capitalis'].sum()\n",
    "cap_mentions_in_cap = df.loc[df['subreddit_s']==0,'capitalis'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,6))\n",
    "barWidth = 0.25\n",
    "\n",
    "soc = [soc_mentions_in_soc, soc_mentions_in_cap]\n",
    "cap = [cap_mentions_in_soc, cap_mentions_in_cap]\n",
    "\n",
    "s = np.arange(len(soc))\n",
    "c = [x + barWidth for x in s]\n",
    "\n",
    "plt.bar(s, soc, color='indigo', width=barWidth, edgecolor='white', label='r/Socialism',alpha=0.6)\n",
    "plt.bar(c, cap, color='skyblue', width=barWidth, edgecolor='white', label='r/Capitalism',alpha=0.6)\n",
    "plt.xticks([r + barWidth/2 for r in range(len(soc))], ['Socialism Mentions', 'Capitalism Mentions'],fontsize=14)\n",
    "plt.yticks(range(0,3501,500),fontsize=12)\n",
    "plt.title('Mentions of Socialism and Captialism by Subreddit',fontsize=18,pad=10)\n",
    "\n",
    "plt.legend(loc='upper center',fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as one might expect, users are more likely to mention the topic of the subreddit rather than the opposing ideology. However, these subreddits often namecheck each other which also makes sense if you think about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "barWidth = 0.3\n",
    "\n",
    "soc = [df.loc[df['subreddit_s']==1,'vader_pos'].mean(),df.loc[df['subreddit_s']==1,'vader_neu'].mean(), \\\n",
    "       df.loc[df['subreddit_s']==1,'vader_neg'].mean(),df.loc[df['subreddit_s']==1,'vader_compound'].mean()]\n",
    "cap = [df.loc[df['subreddit_s']==0,'vader_pos'].mean(),df.loc[df['subreddit_s']==0,'vader_neu'].mean(), \\\n",
    "       df.loc[df['subreddit_s']==0,'vader_neg'].mean(),df.loc[df['subreddit_s']==0,'vader_compound'].mean()]\n",
    "\n",
    "s = np.arange(len(soc))\n",
    "c = [x + barWidth for x in s]\n",
    "\n",
    "plt.bar(s, soc, color='indigo', width=barWidth, edgecolor='white', label='r/Socialism',alpha=0.6)\n",
    "plt.bar(c, cap, color='skyblue', width=barWidth, edgecolor='white', label='r/Capitalism',alpha=0.6)\n",
    "plt.xticks([r + barWidth/2 for r in range(len(soc))],\n",
    "           ['Positivity', 'Neutrality','Negativity','Compound'],fontsize=14)\n",
    "plt.yticks([0,0.2,0.4,0.6,0.8,1],fontsize=12)\n",
    "plt.title('Average Sentiment Scores by Subreddit',fontsize=18,pad=10)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to some of the other charts, this isn't particularly encouraging because the scores are largely very similar. If there are any patterns here, it would be that posts in r/Capitalism are less neutral than those in r/Socialism, but only by the slightest bit. \n",
    "\n",
    "**Exploratory conclusions:** After charting out a few things here, I am not feeling very confident that I'll be able to make a very accurate classification model. However, the models will be much better than I am at detecting patterns in the way all of these features work together in aggregate so I have not yet lost hope!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Now I'm going to start modeling! I'm going to get started by establishing our features and target variable, executing a train/test split, and creating a few functions that I think will come in handy throughout this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df.drop(columns='subreddit_s')\n",
    "y = df['subreddit_s']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.2,stratify=y,random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(model):\n",
    "    return print('\\n',\n",
    "                 'Score for Training Data:', model.score(X_train,y_train),'\\n', \\\n",
    "                 'Score for Testing Data:', model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats(model,coef_or_importances,top_n = None):\n",
    "    if coef_or_importances == 'coef':\n",
    "        f = model.coef_[0]\n",
    "    else:\n",
    "        f = model.feature_importances_\n",
    "        \n",
    "    tf_df = pd.DataFrame()\n",
    "    tf_df['feat'] = X.columns\n",
    "    tf_df['coef'] = [abs(coef) for coef in f]\n",
    "    top_feats = tf_df.sort_values('coef',ascending=False)['feat']\n",
    "    \n",
    "    return list(top_feats[:top_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_strengths(model,coef_or_importances,feats,output):\n",
    "    if coef_or_importances == 'coef':\n",
    "        f = model.coef_[0]\n",
    "    else:\n",
    "        f = model.feature_importances_\n",
    "        \n",
    "    all_feat_dict = {X.columns[i]: f[i]  for i in range(len(X.columns))}\n",
    "    \n",
    "    if type(feats) == list:\n",
    "        feat_dict = {feat: all_feat_dict[feat] for feat in feats}\n",
    "        strength_list = [all_feat_dict[feat] for feat in feats]\n",
    "    else:\n",
    "        feat_dict = {feat: all_feat_dict[feat]}\n",
    "        strength_list = all_feat_dict[feat]\n",
    "    \n",
    "    if output == 'list':\n",
    "        return strength_list\n",
    "    else:\n",
    "        return feat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline accuracy\n",
    "\n",
    "This is the score to beat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(y.value_counts(normalize=True))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "I'm going to try models out in order of increasing complexity (in my mind at least) so the first one is Logistic Regression. Simple but powerful. Let's see how it goes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score for Training Data: 0.8107761385503528 \n",
      " Score for Testing Data: 0.752991452991453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elliotrichardson/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression(random_state = 26)\n",
    "logr.fit(X_train,y_train)\n",
    "scores(logr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Every parameter I added only lowered the scores so I'm keeping it extra simple for this one. It's definitely a bit overfit but it's encouraging to see the most simple model outperform the baseline by almost 50%. I'm going to capture the most important features from this model for later use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_tf = top_feats(logr,'coef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors\n",
    "\n",
    "Now I'm going to try a KNeighborsClassifier. I performed a GridSearch to find the best parameters and then recreated that model below in order to compare test and train scores. I think this will be my workflow for the models to come!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors': [3,5,9,15],\n",
    "    'algorithm': ['auto','ball_tree','kd_tree','brute'],\n",
    "    'p': [1,2]\n",
    "}\n",
    "\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), param_grid = knn_params, cv = 5)\n",
    "\n",
    "knn_grid.fit(X_train_sc,y_train)\n",
    "print(knn_grid.best_score_)\n",
    "knn_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(algorithm = 'ball_tree', n_neighbors = 15, p = 1)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "scores(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was significantly worse than the LogisticRegression so I'm just going to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Now for decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_params = {\n",
    "    'max_depth': [None, 2, 3, 4],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "    'min_samples_leaf': [2, 3, 4, 5],\n",
    "    'ccp_alpha': [0, 0.001, 0.01, 0.1]}\n",
    "\n",
    "dt_grid = GridSearchCV(estimator = DecisionTreeClassifier(random_state = 26),\n",
    "                       param_grid = dt_params,\n",
    "                       cv = 5,\n",
    "                       verbose = 1)\n",
    "\n",
    "dt_grid.fit(X_train,y_train)\n",
    "\n",
    "print(dt_grid.best_score_)\n",
    "dt_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(min_samples_leaf = 5,\n",
    "                           min_samples_split = 10,\n",
    "                           ccp_alpha = 0.001,\n",
    "                           random_state = 26)\n",
    "\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "scores(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one also performed worse than the LogisticRegression model so I am going to keep going. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging classifier\n",
    "\n",
    "Maybe bagging with help bring our scores up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bag = BaggingClassifier(random_state = 26)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "\n",
    "scores(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did help a bit but we are still doing worse than the LogisticRegression model :^("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 1, 2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(estimator = RandomForestClassifier(random_state = 26),\n",
    "                       param_grid = rf_params,\n",
    "                       cv=5)\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(rf_grid.best_score_)\n",
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score for Training Data: 0.9985033140902287 \n",
      " Score for Testing Data: 0.7384615384615385\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 150, random_state = 26)\n",
    "\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "scores(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, this is our best one so far! It's only a slightly higher test score than the LogisticRegression and it's very overfit, but I'm pretty pleased. I'm going to capture the top features below for later use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tf = top_feats(rf,'importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees\n",
    "\n",
    "Now for extra trees! Maybe we can do even better than the RandomForest score. However, there are a ton of hyperparameters to tune and my computer is running very slowly so I'm going to do it in two separate GridSearches which probably isn't ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_params = {'n_estimators': [50,100,200],\n",
    "             'max_depth': [None,2,4],\n",
    "             'min_samples_split': [2,4,6],\n",
    "             'ccp_alpha': [0,0.001,0.01]\n",
    "    \n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(estimator = ExtraTreesClassifier(random_state = 26),\n",
    "                      param_grid = et_params,\n",
    "                      cv = 5)\n",
    "\n",
    "et_grid.fit(X_train,y_train)\n",
    "\n",
    "print(et_grid.best_score_)\n",
    "et_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_params = {'max_leaf_nodes': [None,20,50],\n",
    "             'min_impurity_decrease': [0,0.001,0.01],\n",
    "             'warm_start': [True,False],\n",
    "             'bootstrap': [True,False]\n",
    "    \n",
    "}\n",
    "\n",
    "et_grid = GridSearchCV(estimator = ExtraTreesClassifier(min_samples_split = 5,\n",
    "                                                        n_estimators = 200,\n",
    "                                                        random_state = 26),\n",
    "                      param_grid = et_params,\n",
    "                      cv = 5)\n",
    "\n",
    "et_grid.fit(X_train,y_train)\n",
    "\n",
    "print(et_grid.best_score_)\n",
    "et_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score for Training Data: 0.9980756895445798 \n",
      " Score for Testing Data: 0.7555555555555555\n"
     ]
    }
   ],
   "source": [
    "et = ExtraTreesClassifier(min_samples_split = 5, n_estimators = 200,\n",
    "                          warm_start=True, random_state = 26)\n",
    "\n",
    "et.fit(X_train,y_train)\n",
    "\n",
    "scores(et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo, a new best! Maybe the separate GridSearches worked after all. Definitely capturing the top features here for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_tf = top_feats(et,'importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "## Ada Boost\n",
    "\n",
    "I'm going to use the AdaBoost to try and boost our ExtraTrees score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ada_params = {'learning_rate': [.9,1,1.1]}\n",
    "\n",
    "ada_grid = GridSearchCV(AdaBoostClassifier(base_estimator = et), param_grid = ada_params)\n",
    "\n",
    "ada_grid.fit(X_train,y_train)\n",
    "\n",
    "print(ada_grid.best_score_)\n",
    "ada_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator = et,\n",
    "                        learning_rate = .9,\n",
    "                         algorithm = 'SAMME',\n",
    "                         random_state = 26\n",
    "                    )\n",
    "\n",
    "ada.fit(X_train,y_train)\n",
    "scores(ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're on a roll; another new best. As always, capturing these coefficients for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_tf = top_feats(ada,'importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost\n",
    "\n",
    "Now I'm going to compare that performance to the GradientBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_params = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [.08, .1, .12]\n",
    "}\n",
    "gb_grid = GridSearchCV(GradientBoostingClassifier(), param_grid=gb_params, cv=3)\n",
    "gb_grid.fit(X_train, y_train)\n",
    "print(gb_grid.best_score_)\n",
    "gb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(learning_rate = 0.08, max_depth = 4, n_estimators = 150, random_state = 26)\n",
    "\n",
    "gb.fit(X_train,y_train)\n",
    "\n",
    "scores(gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best testing scores but not *the* best. Probably our least overfit model though! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_tf = top_feats(gb,'importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "\n",
    "Now I'm going to try and pool the wisdom of all 5 of our top performing models (those with 70+ testing scores) by using the VotingClassifier. I have high hopes for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('rf', rf),\n",
    "    ('et', et),\n",
    "    ('ada', ada),\n",
    "    ('gb', gb),\n",
    "])\n",
    "\n",
    "vote.fit(X_train,y_train)\n",
    "\n",
    "scores(vote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAY highest testing score so far! It's definitely quite overfit so I'm going to try using the all of the captured features from earlier models to narrow down the features and recreate this VotingClassifier with a more pared down `X_train`.\n",
    "\n",
    "Below I am retrieving the test scores for each of my five top models and using them to weight each of them in my next VotingClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_scores = [model.score(X_test,y_test) for model in [logr,rf,et,ada,gb]]\n",
    "print(list(zip(['Logreg','RF','ET','ADA','GB'],model_scores)))\n",
    "weights = [score/sum(model_scores) for score in model_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to use those weights for another purpose as well! I'm going to slightly prioritize the strongest features from the best models as I narrow down my X columns. That's what I'm doing below with this for loop! I ended up with 800 features for my next VotingClassifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_num_of_feat = 1000\n",
    "feats = []\n",
    "\n",
    "for tf,weight in zip([logr_tf,rf_tf,et_tf,gb_tf,ada_tf],weights):\n",
    "    feats_taken = math.ceil(target_num_of_feat * 2 * weight)\n",
    "    for i in range(feats_taken):\n",
    "        feats.append(tf[i])\n",
    "        \n",
    "lim_feats = set(feats)\n",
    "len(lim_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I am changing the features, I need to re-establish my X matrices and reinstantiate the models so they aren't still fit to the 1200+ features I used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df[lim_feats]\n",
    "\n",
    "X2_train,X2_test,y_train,y_test = train_test_split(X2,y,test_size=.2,\n",
    "                                                   stratify=y,random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reinstantiating my top performing models\n",
    "logr2 = LogisticRegression()\n",
    "rf2 = RandomForestClassifier(n_estimators = 150, random_state = 26)\n",
    "et2 = ExtraTreesClassifier(min_samples_split = 5, n_estimators = 200,\n",
    "                          warm_start=True, random_state = 26)\n",
    "ada2 = AdaBoostClassifier(base_estimator = et,\n",
    "                        learning_rate = .9,\n",
    "                        algorithm = 'SAMME')\n",
    "gb2 = GradientBoostingClassifier(learning_rate = 0.08, max_depth = 4, n_estimators = 150)\n",
    "\n",
    "for model in [logr2,rf2,et2,ada2,gb2]:\n",
    "    model.fit(X2_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to try this VotingClassifier again! The only difference in hyperparameters this time is that I added the weights that I calculated before. They only ended up very slightly different from one another since all the scores were so close, but I thought it was a cool way to prioritize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vote2 = VotingClassifier([\n",
    "    ('logr',logr2),\n",
    "    ('rf', rf2),\n",
    "    ('et', et2),\n",
    "    ('ada', ada2),\n",
    "    ('gb', gb2)],\n",
    "    weights = weights)\n",
    "vote2.fit(X2_train,y_train)\n",
    "\n",
    "scores(vote2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay that helped a bit! We went from 0.772 to 0.774 which is a small little jump but still exciting to see. I will note that I also tried this with a smaller group of features and got a lower score than before so I thought this might be the sweet spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "So my best model was a VotingClassifier that utilized the wisdom of a LogisticRegression model, a RandomForests model, an ExtraTrees model, an AdaBoost model with ExtraTrees as the base estimator, and a GradientBoost. The final testing score was 0.774, meaning we outperformed the baseline accuracy by 51.7%. I'm pretty pleased! Now I'm going to find a list of the most important features in this model to come to some findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top10s = []\n",
    "lists_of_strengths = []\n",
    "\n",
    "# using my functions created earlier to create a list of top 10 \n",
    "# features and their respective strengths for each of the 5 models\n",
    "for model, c_or_i in zip([logr2,rf2,et2,ada2,gb2],['coef','importances','importances','importances','importances']):\n",
    "    top10 = top_feats(model, c_or_i, top_n = 10)\n",
    "    top10s.append(top10)\n",
    "    strs = feat_strengths(model,c_or_i,top10,'list')   \n",
    "    lists_of_strengths.append(strs)\n",
    "\n",
    "# gonna put them in a dataframe so I can look at them that way\n",
    "feat_df = pd.DataFrame()\n",
    "\n",
    "models = ['LogReg','RF','ET','ADA','GB']\n",
    "\n",
    "for i in range(5):\n",
    "    model = models[i]\n",
    "    top10 = top10s[i]\n",
    "    strs = lists_of_strengths[i]\n",
    "    feat_df[model+'_coefs'] = top10\n",
    "    feat_df[model+'_strengths'] = strs\n",
    "    \n",
    "    \n",
    "feat_df.index = list(range(1,11))\n",
    "feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to create a larger list that includes all of the top 10 features from all of the models. Then I'll calculate the average coefficient for each feature across all 5 models so we can get a sense of how they generally affecetd predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feat_list = []\n",
    "\n",
    "for top10 in top10s:\n",
    "    for feature in top10:\n",
    "        top_feat_list.append(feature)\n",
    "        \n",
    "top_feat_list = list(set(top_feat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_strs = []\n",
    "\n",
    "for feat in top_feat_list:\n",
    "    strs = []\n",
    "    for model, c_or_i in zip([logr2,rf2,et2,ada2,gb2],\n",
    "                             ['coef','importances','importances','importances','importances']):\n",
    "        strength = feat_strengths(model,c_or_i,top_feat_list,'dict')[feat]\n",
    "        strs.append(strength)\n",
    "        \n",
    "    avg_strs.append(np.mean(strs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some of the functions I created earlier, I calculated the average coefficients for each of these features and now it's time to visualize them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_strs = pd.DataFrame()\n",
    "coef_strs['coef'] = top_feat_list\n",
    "coef_strs['strength'] = avg_strs\n",
    "\n",
    "coef_strs.sort_values('strength',ascending=True,inplace=True)\n",
    "\n",
    "colors = ['indigo' if strength > 0 else 'skyblue' for strength in coef_strs['strength']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.barh(coef_strs['coef'],coef_strs['strength'],color = colors,alpha=0.6)\n",
    "plt.title('Strongest Features Across All Models',fontsize=20)\n",
    "plt.ylabel('Feature',fontsize=18)\n",
    "plt.yticks(coef_strs['coef'],fontsize=14)\n",
    "plt.xlabel('Coefficient (Features with larger coefficients are associated with r/Socialism)',fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading this far! I hope it was interesting and useful. :^)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Elliot Richardson\n",
    "\n",
    "*This is the notebook in which I do my data acquisition, cleaning, and processing. For analysis, modeling, and findings, please check out the other jupyter notebook in this repository.*\n",
    "\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "I. [Requesting content from subreddits](#Requesting-content-from-the-subreddits)\n",
    "\n",
    "II. [Formatting requested content](#Formatting-requested-posts)\n",
    "\n",
    "III. [Initial processing and cleaning](#Processing-part-1)\n",
    "\n",
    "IV. [Processing and word selection](#Processing-part-2)\n",
    "\n",
    "V. [Exporting for analysis](#Exporting-features-and-target-variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages/libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requesting content from the subreddits\n",
    "\n",
    "I created a function to grab the maximum number of posts allowed by the Pushshift API so that I could automate the process with a while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(subreddit,n=10,to_extend=None):\n",
    "    \"\"\"By default returns a list of 1,000 posts from the subreddit passed unless errors encountered\"\"\"\n",
    "    \"\"\"If no errors encountered, it will return n*100 posts\"\"\"\n",
    "    \"\"\"If you are tacking more posts on to an existing list, pass the list through the to_extend parameter\"\"\"\n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    \n",
    "    # if no existing list given to tack these posts onto\n",
    "    if to_extend == None:\n",
    "        posts = []\n",
    "        \n",
    "    # if an existing list is given\n",
    "    else:        \n",
    "        posts = to_extend\n",
    "              \n",
    "    # extending list by n sets of 100 posts if no errors encountered\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            # if there are posts already in the list\n",
    "            before = posts[-1]['created_utc']\n",
    "\n",
    "        except:\n",
    "            # if there aren't any posts in list yet\n",
    "            before = None\n",
    "\n",
    "    # tacking on the 100 posts before the last one already in the list\n",
    "        params = {'subreddit': subreddit,'size': 100,'before': before}\n",
    "        res = requests.get(url,params)\n",
    "        try:\n",
    "            data = res.json()\n",
    "            posts.extend(data['data'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the above function and while loops to get at least 2,500 substantive posts from each subreddit. I exported the posts and their corresponding data to CSVs and read them back in. I'm going to do modeling and analysis in another notebook so I don't have to run this whole thing everytime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s_posts = get_posts('Socialism')\n",
    "s_df = pd.DataFrame(s_posts)\n",
    "s_actual_posts = ~s_df['selftext'].isin(['removed','','[removed]','deleted'])\n",
    "s_not_null = ~s_df['selftext'].isnull()\n",
    "\n",
    "while s_df[s_actual_posts & s_not_null].shape[0] < 2800:\n",
    "    s_posts = get_posts('Socialism',n=50,to_extend=s_posts)\n",
    "    s_df = pd.DataFrame(s_posts)\n",
    "    s_actual_posts = ~s_df['selftext'].isin(['removed','','[removed]','deleted'])\n",
    "    s_not_null = ~s_df['selftext'].isnull()\n",
    "    time.sleep(5)\n",
    "    print(s_df[s_actual_posts & s_not_null].shape)\n",
    "\n",
    "s_df = s_df.loc[(s_actual_posts & s_not_null),:]\n",
    "s_df.duplicated('selftext').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_posts = get_posts('Capitalism')\n",
    "c_df = pd.DataFrame(c_posts)\n",
    "c_actual_posts = ~c_df['selftext'].isin(['removed','','[removed]','deleted'])\n",
    "c_not_null = ~c_df['selftext'].isnull()\n",
    "\n",
    "while c_df[c_actual_posts & c_not_null].shape[0] < 2800:\n",
    "    c_posts = get_posts('Capitalism',n=50,to_extend=c_posts)\n",
    "    c_df = pd.DataFrame(c_posts)\n",
    "    c_actual_posts = ~c_df['selftext'].isin(['removed','','[removed]','deleted'])\n",
    "    c_not_null = ~c_df['selftext'].isnull()\n",
    "    time.sleep(5)\n",
    "    print(c_df[c_actual_posts & c_not_null].shape)\n",
    "\n",
    "c_df = c_df.loc[(c_actual_posts & c_not_null),:]\n",
    "c_df.duplicated('selftext').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_df.to_csv('./data/socialism.csv',index=False)\n",
    "c_df.to_csv('./data/capitalism.csv',index=False)\n",
    "\n",
    "s = pd.read_csv('./data/socialism.csv')\n",
    "c = pd.read_csv('./data/capitalism.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting requested posts\n",
    "Now that I have 2,500+ substantive posts from each subreddit, I am going to combine them into one dataframe by eliminating any columns that they don't have in common and adding a binary indicator of the sourced subreddit. Then I'm going to do some work to clean and process the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_cols = set(s.columns)\n",
    "c_cols = set(c.columns)\n",
    "\n",
    "s.drop(columns=s_cols.difference(c_cols),inplace=True)\n",
    "c.drop(columns=c_cols.difference(s_cols),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2985, 79), (2862, 79), True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape,c.shape,set(s.columns) == set(c.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c.loc[:,s.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['subreddit_s'] = 1\n",
    "c['subreddit_s'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5847, 80)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([s,c])\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "df.to_csv('./data/unprocessed_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing part 1\n",
    "After all of that, I ended up with a dataframe of 5,847 rows and 80 columns. That should be plenty of information to train a model! I'm going to start creating some features that measure the structure and sentiment of each post before breaking down the posts into their most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/unprocessed_df.csv')\n",
    "df['all_text'] = [df.loc[i,'title'] + ' '+df.loc[i,'selftext'] for i in range(df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_text'] = df['all_text'].map(lambda x: re.sub('http[s]://\\S+','',x))\n",
    "\n",
    "df['all_text'] = df['all_text'].str.replace('publsih','publish') # just a typo i noticed in the head()\n",
    "\n",
    "for s in ['\\n','\\xa0','&gt;']:\n",
    "    df['all_text'] = df['all_text'].map(lambda x: x.replace(s,''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "senter = PunktSentenceTokenizer()\n",
    "df['sentences'] = [0] * df.shape[0]\n",
    "df['avg_sent_len'] = [0] * df.shape[0]\n",
    "df['words'] = [0] * df.shape[0]\n",
    "df['avg_word_len'] = [0] * df.shape[0]\n",
    "\n",
    "\n",
    "for i,text in enumerate(df['all_text']):\n",
    "    sentences = senter.sentences_from_text(text)\n",
    "    num_sent = len(sentences)\n",
    "    sent_len = []\n",
    "    for sent in sentences:\n",
    "        words = sent.split(' ')\n",
    "        sent_len.append(len(words))\n",
    "        word_len = []\n",
    "        for word in words:\n",
    "            word_len.append(len(word))\n",
    "    \n",
    "    df.loc[i,'sentences'] = num_sent\n",
    "    df.loc[i,'avg_sent_len'] = np.mean(sent_len)\n",
    "    df.loc[i,'words'] = sum(sent_len)\n",
    "    df.loc[i,'avg_word_len'] = np.mean(word_len)\n",
    "    \n",
    "    if (i % 1000) == 0:\n",
    "        print(i)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_text'] = df.all_text.map(lambda x: ''.join([y if y in string.ascii_lowercase else ' ' for y in list(str(x).lower())]))\n",
    "df['all_text'] = df.all_text.map(lambda x: x.replace('-',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "parsed_quotes = []\n",
    "for i, parsed in enumerate(en_nlp.pipe(df.all_text.values, batch_size=50, n_threads=4)):\n",
    "    assert parsed.is_parsed\n",
    "    if (i % 1000) == 0:\n",
    "        print(i)\n",
    "    parsed_quotes.append(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "unique_pos = []\n",
    "for parsed in parsed_quotes:\n",
    "    unique_pos.extend([t.pos_ for t in parsed])\n",
    "unique_pos = np.unique(unique_pos)\n",
    "\n",
    "for pos in unique_pos:\n",
    "    df[pos+'_prop'] = 0.\n",
    "\n",
    "for i, parsed in enumerate(parsed_quotes):\n",
    "    if (i % 1000) == 0:\n",
    "        print(i)\n",
    "    parsed_len = len(parsed)\n",
    "    for pos in unique_pos:\n",
    "        count = len([x for x in parsed if x.pos_ == pos])\n",
    "        try:\n",
    "            df.loc[i, pos+'_prop'] = float(count)/parsed_len    \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['vader_neg'] = 0\n",
    "df['vader_pos'] = 0\n",
    "df['vader_neu'] = 0\n",
    "df['vader_compound'] = 0\n",
    "\n",
    "for i, t in enumerate(df.all_text.values):\n",
    "    vs = analyzer.polarity_scores(t)\n",
    "    df.loc[i, 'vader_neg'] = vs['neg']\n",
    "    df.loc[i, 'vader_pos'] = vs['pos']\n",
    "    df.loc[i, 'vader_neu'] = vs['neu']\n",
    "    df.loc[i, 'vader_compound'] = vs['compound']\n",
    "    if (i % 1000) == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_df = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['thumbnail_height', 'thumbnail_width', 'url_overridden_by_dest',\n",
       "       'crosspost_parent', 'crosspost_parent_list', 'author_flair_template_id',\n",
       "       'author_flair_text_color', 'author_flair_background_color', 'media',\n",
       "       'media_embed', 'secure_media', 'secure_media_embed', 'media_metadata',\n",
       "       'link_flair_text', 'banned_by', 'edited', 'suggested_sort',\n",
       "       'distinguished', 'author_cakeday', 'subreddit_s', 'all_text',\n",
       "       'sentences', 'avg_sent_len', 'words', 'avg_word_len', 'ADJ_prop',\n",
       "       'ADP_prop', 'ADV_prop', 'AUX_prop', 'CCONJ_prop', 'DET_prop',\n",
       "       'INTJ_prop', 'NOUN_prop', 'NUM_prop', 'PART_prop', 'PRON_prop',\n",
       "       'PROPN_prop', 'PUNCT_prop', 'SCONJ_prop', 'SPACE_prop', 'SYM_prop',\n",
       "       'VERB_prop', 'X_prop', 'vader_neg', 'vader_pos', 'vader_neu',\n",
       "       'vader_compound'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,[  'subreddit_s',\n",
    "                 'all_text',\n",
    "                 'sentences',\n",
    "                 'avg_sent_len',\n",
    "                 'words',\n",
    "                 'avg_word_len',\n",
    "                 'ADJ_prop',\n",
    "                 'ADP_prop',\n",
    "                 'ADV_prop',\n",
    "                 'AUX_prop',\n",
    "                 'CCONJ_prop',\n",
    "                 'DET_prop',\n",
    "                 'INTJ_prop',\n",
    "                 'NOUN_prop',\n",
    "                 'NUM_prop',\n",
    "                 'PART_prop',\n",
    "                 'PRON_prop',\n",
    "                 'PROPN_prop',\n",
    "                 'PUNCT_prop',\n",
    "                 'SCONJ_prop',\n",
    "                 'SPACE_prop',\n",
    "                 'SYM_prop',\n",
    "                 'VERB_prop',\n",
    "                 'X_prop',\n",
    "                 'vader_neg',\n",
    "                 'vader_pos',\n",
    "                 'vader_neu',\n",
    "                 'vader_compound']]\n",
    "\n",
    "df.to_csv('./data/processed_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing part 2\n",
    "\n",
    "Now that I have done some syntax and sentiment analysis, I want to narrow down the huge list of words in these posts to the most important stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/processed_df.csv')\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(df['all_text'])\n",
    "\n",
    "words_cv = cv.transform(df['all_text'])\n",
    "words_df = pd.DataFrame(words_cv.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5847, 26518)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "word_list = words_df.columns\n",
    "stemmed_words = list(set([stemmer.stem(word) for word in word_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26518, 17083)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list),len(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We narrowed the list pretty significantly. I'm going to keep pushing forward to narrow even more! I'm going to transfer the stems into a dictionary so that I can edit and add stems as I see fit. You can see some of the words I decided to tweak below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_dict = {}\n",
    "\n",
    "for word in word_list:\n",
    "    stem = stemmer.stem(word)\n",
    "    stem_dict[word]= (stem)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_dict['capitalism'] = 'capitalis'\n",
    "stem_dict['capitalist'] = 'capitalis'\n",
    "stem_dict['capitalistic'] = 'capitalis'\n",
    "stem_dict['capitalists'] = 'capitalis'\n",
    "stem_dict['capitalize'] = 'capitalize'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_dict['socialism'] = 'socialis'\n",
    "stem_dict['socialiam'] = 'socialis'\n",
    "stem_dict['socialist'] = 'socialis'\n",
    "stem_dict['socialists'] = 'socialis'\n",
    "stem_dict['socialized'] = 'socialize'\n",
    "stem_dict['socialismvcapitalism'] = 'socialis versus capitalis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_dict['communist'] = 'communis'\n",
    "stem_dict['communists'] = 'communis'\n",
    "stem_dict['communism'] = 'communis'\n",
    "stem_dict['communicate'] = 'communicat'\n",
    "stem_dict['communications'] = 'communicat'\n",
    "stem_dict['communication'] = 'communicat'\n",
    "stem_dict['communicated'] = 'communicat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_dict['youtu'] = 'youtub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have stems assigned to all the non-stop words in my posts, I'm going to create strings with only the stems for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "df['stemmed'] = [np.nan]*df.shape[0]\n",
    "\n",
    "for i, text in enumerate(list(df['all_text'])):\n",
    "    stemmed = []\n",
    "    for word in text.split(' '):\n",
    "        if word in stem_dict.keys():\n",
    "            stem = stem_dict[word]\n",
    "            stemmed.append(stem)\n",
    "            \n",
    "    df.loc[i,'stemmed'] = ' '.join(stemmed)\n",
    "    if (i % 1000) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences have now been narrowed down to their stems, so I'm going to vectorize again and use TF IDF to figure out which words will be most important in my modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(df['stemmed'])\n",
    "\n",
    "words_cv = cv.transform(df['stemmed'])\n",
    "\n",
    "stems_df = pd.DataFrame(words_cv.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_string = ''\n",
    "c_string = ''\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    string = df.loc[i,'stemmed']\n",
    "    if df.loc[i,'subreddit_s'] == 1:\n",
    "        s_string += ' '+string\n",
    "    else: \n",
    "        c_string += ' '+string\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tvec.fit([s_string,c_string])\n",
    "\n",
    "tv  = pd.DataFrame(tvec.transform([s_string, c_string]).todense(),\n",
    "                   columns=tvec.get_feature_names(),\n",
    "                   index=['socialism', 'capitalism'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm selecting the top 1,000 words from each subreddit as features as I think these will be the most useful in a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1197"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_words = set(tv.T.sort_values('socialism', ascending=False).head(1000).T.columns)\n",
    "c_words = set(tv.T.sort_values('capitalism', ascending=False).head(1000).T.columns)\n",
    "selected_words = list(s_words.union(c_words))\n",
    "selected_words.sort()\n",
    "len(selected_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting features and target variable\n",
    "Awesome! I have a bunch of information about the grammatical makeup, sentiment, and vocabulary used in these posts. Now I'm going to combine my features and target variable into one clean dataframe for use in my other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = df.drop(columns = ['all_text','stemmed'])\n",
    "\n",
    "for word in selected_words:\n",
    "    model_df[word] = stems_df[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_csv('./data/model_df.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
